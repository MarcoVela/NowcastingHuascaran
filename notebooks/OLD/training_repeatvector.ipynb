{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.backend import expand_dims, repeat_elements\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_STEPS = 32\n",
    "OUTPUT_STEPS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\"\"\"\n",
    "\n",
    "    def __init__(self, paths, batch_size=16, dim=(64, 64),\n",
    "                 shuffle=True, input_steps=16, output_steps=8):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.dim = dim\n",
    "        self.file_batch_size = 2048\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = self.file_batch_size // self.batch_size\n",
    "        self.file_paths = paths\n",
    "        self.shuffle = shuffle\n",
    "        self.current_file_loaded = (None, None)\n",
    "        self.steps = (input_steps, output_steps)\n",
    "        self.indexes = np.arange(len(self.file_paths) * int(self.file_batch_size / self.batch_size))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def load_file(self, i):\n",
    "        with h5py.File(self.file_paths[i], \"r\") as f:\n",
    "            self.current_file_loaded = i, (f[\"data/X\"][:], f[\"data/y\"][:])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.file_paths) * self.n_batches\n",
    "        # TODO: Arreglar esto\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        file_index = index // self.n_batches\n",
    "        current_file_index, _ = self.current_file_loaded\n",
    "        if file_index != current_file_index:\n",
    "            self.load_file(file_index)\n",
    "        _, (X, y) = self.current_file_loaded\n",
    "        index_inside_file = index % self.n_batches\n",
    "        i = index_inside_file * self.batch_size\n",
    "        inp, out = self.steps\n",
    "        X = np.log(X[i:(i + self.batch_size), -inp:] + 1.0, dtype=np.float32)\n",
    "        y = y[i:(i + self.batch_size), :out].astype(np.float32)\n",
    "        y[y > 1.0] = np.float32(1.0)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.file_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            np.random.shuffle(self.file_paths)\n",
    "            self.current_file_loaded = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def get_compiled_model():\n",
    "\n",
    "    inp = layers.Input(shape=(INPUT_STEPS, 64, 64, 1))\n",
    "\n",
    "    # Probar normalizar dividiendo entre 300\n",
    "    # Probar normalizar log(x+1)\n",
    "    \n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=48,\n",
    "        kernel_size=(5, 5),\n",
    "        #padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\",\n",
    "    )(inp)\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        #padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "    x = Lambda(lambda x: repeat_elements(expand_dims(x, axis=1), OUTPUT_STEPS, 1))(x)\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1, 1),\n",
    "        #padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=64,\n",
    "        kernel_size=(1, 1),\n",
    "        #padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters=64, kernel_size=(3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "    )(x)\n",
    "\n",
    "\n",
    "    model = Model(inp, x)\n",
    "    model.compile(\n",
    "        loss=BinaryCrossentropy(), \n",
    "        optimizer=Adam(),\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = \"../data/exp_pro/GLM-L2-LCFA_8km_5m_boxes/2019/BATCH_SIZE=2048_CLUSTERING_MIN_CLUSTER_SIZE=512_CLUSTERING_RADIUS=24_CLUSTERING_THRESHOLD=0.0_CLUSTERING_TIME_FACTOR=8_HEIGHT=64_RNG_SEED=42_SLIDING_WINDOW_STEPS=4_TIME_DELTA_AFTER=4_TIME_DELTA_BEFORE=6_TIME_IN=32_TIME_OUT=16_WIDTH=64/h5/\"\n",
    "files = np.array([os.path.join(folder, x) for x in os.listdir(folder)][:-1])\n",
    "\n",
    "train_val_test_split = (.7, .2, .1)\n",
    "\n",
    "def split(arr, splits):\n",
    "    arr = np.array(arr)\n",
    "    idxs = np.arange(len(arr))\n",
    "    np.random.shuffle(idxs)\n",
    "    n_splits = list(int(len(arr)*x) for x in splits)\n",
    "    datasets = []\n",
    "    start = 0\n",
    "    n_splits[-1] += len(arr) - sum(n_splits)\n",
    "    n_splits = tuple(n_splits)\n",
    "    for split in n_splits:\n",
    "        datasets.append(arr[idxs[start:start+split]])\n",
    "        start += split\n",
    "    return tuple(datasets)\n",
    "\n",
    "train_files, val_files, test_files = split(files, train_val_test_split)\n",
    "\n",
    "train_generator = DataGenerator(train_files, input_steps=INPUT_STEPS, output_steps=OUTPUT_STEPS)\n",
    "val_generator = DataGenerator(val_files, input_steps=INPUT_STEPS, output_steps=OUTPUT_STEPS)\n",
    "test_generator = DataGenerator(test_files, input_steps=INPUT_STEPS, output_steps=OUTPUT_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0, 1>\n",
    "# [1, 5>\n",
    "# [5, ...>\n",
    "# [..., 80>\n",
    "# [80, inf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 64, 64, 1)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 32, 60, 60, 48)    235392    \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 58, 58, 64)        258304    \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 16, 58, 58, 64)    0         \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 16, 58, 58, 64)    33024     \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 16, 58, 58, 64)    33024     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 58, 58, 64)    36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,672\n",
      "Trainable params: 596,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TerminateOnNaN\n",
    "\n",
    "\n",
    "filepath = \"saved-model-{epoch:02d}-{val_loss:.6f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='auto', save_weights_only=False)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "terminate_nan = TerminateOnNaN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        reduce_lr,\n",
    "        checkpoint,\n",
    "        terminate_nan,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
